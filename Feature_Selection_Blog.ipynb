{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Data\n",
    "data = load_breast_cancer()\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df['target'] = data.target\n",
    "\n",
    "#used columns\n",
    "target = 'target'\n",
    "used_cols = [c for c in df.columns.tolist() if c not in [target]]\n",
    "X, y = df[used_cols], df[target]\n",
    "\n",
    "#train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.3, \n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive Feature Elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy No Feature Selection: 0.9707602339181286\n",
      "orignial features: 30\n",
      "\n",
      "Accuracy With Feature Selection: 0.9824561403508771\n",
      "Selected features: 7\n"
     ]
    }
   ],
   "source": [
    "#No Feature Selection\n",
    "lr = LogisticRegression(class_weight = 'balanced', solver = 'lbfgs', random_state=42, n_jobs=-1, max_iter=500)\n",
    "lr.fit(X_train, y_train)\n",
    "y_guess = lr.predict(X_train)\n",
    "y_score = lr.predict(X_test)\n",
    "print(\"Accuracy No Feature Selection:\",accuracy_score(y_test, y_score))\n",
    "print('orignial features:', str(len(used_cols)))\n",
    "print()\n",
    "\n",
    "# Recursive Feature Elimination\n",
    "from sklearn.feature_selection import RFE\n",
    "rfe = RFE(lr, n_features_to_select=7)\n",
    "rfe.fit(X_train, y_train)\n",
    "y_guess = rfe.predict(X_train)\n",
    "y_score = rfe.predict(X_test)\n",
    "\n",
    "rfe_support = rfe.get_support()\n",
    "rfe_feature = X.loc[:,rfe_support].columns.tolist()\n",
    "print(\"Accuracy With Feature Selection:\",accuracy_score(y_test, y_score))\n",
    "print('Selected features:', str(len(rfe_feature)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which Features Did we Choose?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mean radius',\n",
       " 'mean concavity',\n",
       " 'worst radius',\n",
       " 'worst compactness',\n",
       " 'worst concavity',\n",
       " 'worst concave points',\n",
       " 'worst symmetry']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfe_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy No Feature Selection: 0.9707602339181286\n",
      "orignial features: 30\n",
      "\n",
      "Accuracy With Feature Selection: 0.9649122807017544\n",
      "Selected features: 23\n"
     ]
    }
   ],
   "source": [
    "#No Feature Selection\n",
    "lr = LogisticRegression(class_weight = 'balanced', solver = 'lbfgs', random_state=42, n_jobs=-1, max_iter=500)\n",
    "lr.fit(X_train, y_train)\n",
    "y_guess = lr.predict(X_train)\n",
    "y_score = lr.predict(X_test)\n",
    "print(\"Accuracy No Feature Selection:\",accuracy_score(y_test, y_score))\n",
    "print('orignial features:', str(len(used_cols)))\n",
    "print()\n",
    "\n",
    "# Sequential Feature Selection\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector\n",
    "sfs = SequentialFeatureSelector(lr, k_features='best', n_jobs=-1)\n",
    "sfs.fit(X_train, y_train)\n",
    "# y_guess = sfs.predict(X_train)\n",
    "features = list(sfs.k_feature_names_)\n",
    "lr.fit(X_train[features], y_train)\n",
    "y_score = lr.predict(X_test[features])\n",
    "\n",
    "#save features\n",
    "print(\"Accuracy With Feature Selection:\",accuracy_score(y_test, y_score))\n",
    "print('Selected features:', str(len(features)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which Features did We Choose?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mean texture',\n",
       " 'mean smoothness',\n",
       " 'mean compactness',\n",
       " 'mean concavity',\n",
       " 'mean concave points',\n",
       " 'mean symmetry',\n",
       " 'mean fractal dimension',\n",
       " 'texture error',\n",
       " 'area error',\n",
       " 'smoothness error',\n",
       " 'compactness error',\n",
       " 'concavity error',\n",
       " 'concave points error',\n",
       " 'symmetry error',\n",
       " 'fractal dimension error',\n",
       " 'worst radius',\n",
       " 'worst texture',\n",
       " 'worst area',\n",
       " 'worst smoothness',\n",
       " 'worst compactness',\n",
       " 'worst concavity',\n",
       " 'worst concave points',\n",
       " 'worst symmetry']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exhaustive Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #No Feature Selection\n",
    "# lr = LogisticRegression(class_weight = 'balanced', solver = 'lbfgs', random_state=42, n_jobs=-1)\n",
    "# lr.fit(X_train, y_train)\n",
    "# y_guess = lr.predict(X_train)\n",
    "# y_score = lr.predict(X_test)\n",
    "# print(\"Accuracy No Feature Selection:\",accuracy_score(y_test, y_score))\n",
    "# print('orignial features:', str(len(used_cols)))\n",
    "# print()\n",
    "\n",
    "# # Exhaustive Feature Elimination\n",
    "# from mlxtend.feature_selection import ExhaustiveFeatureSelector\n",
    "# efs = ExhaustiveFeatureSelector(lr, n_jobs=-1, max_features=5)\n",
    "# efs.fit(X_train, y_train)\n",
    "# # y_guess = sfs.predict(X_train)\n",
    "# features = list(efs.best_feature_names_)\n",
    "# lr.fit(X_train[features], y_train)\n",
    "# y_score = lr.predict(X_test[features])\n",
    "\n",
    "# #save features\n",
    "# print(\"Accuracy With Feature Selection:\",accuracy_score(y_test, y_score))\n",
    "# print('Selected features:', str(len(features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
